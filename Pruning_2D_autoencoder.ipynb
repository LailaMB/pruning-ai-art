{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pruning_2D_autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CEriDkyqvYN"
      },
      "source": [
        "#Pruning 2D Autoencoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DN88abxf2nh"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "import torch.nn.utils.prune as prune\n",
        "import os\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5), (0.5))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dQyo5XAgCyT"
      },
      "source": [
        "# dataset class\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_path, transform=None):\n",
        "        self.image_path = image_path\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return 100\n",
        "\n",
        "    def __getitem__(self, x):\n",
        "        image = plt.imread(self.image_path)\n",
        "        image = TF.to_tensor(image)\n",
        "        s = image.size\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzzxp3GLgEvr"
      },
      "source": [
        "### CHANGE the IMAGE URL\n",
        "\n",
        "IMG_URL = '/content/Famous-portrait-CT_2860.jpeg'\n",
        "\n",
        "dataset = MyDataset(IMG_URL)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DQe8twkgFQL"
      },
      "source": [
        "### Build an autoencoder\n",
        "\n",
        "class autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, stride=1, padding = 0),  # b, 16, 398, 398\n",
        "            nn.ReLU(True),\n",
        "            #nn.MaxPool2d(2, stride=2),  # b,  16, 199, 199\n",
        "            nn.Conv2d(16, 4, 3, stride=2, padding = 0),  # b, 8, 99, 99\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2, padding = 1)  # b, 8, 49, 49\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            \n",
        "            nn.ConvTranspose2d(4, 16, 2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16, 3, 2, stride=2),\n",
        "            nn.Tanh()          \n",
        "            \n",
        "            #nn.ConvTranspose2d(8, 16, 3, stride=2, padding = 0),  # b, 16, 5, 5\n",
        "            #nn.ReLU(True),\n",
        "            #nn.ConvTranspose2d(16, 8, 3, stride=2, padding = 0),  # b, 8, 15, 15\n",
        "            #nn.ReLU(True),\n",
        "            #nn.ConvTranspose2d(8, 1, 5, stride=2, padding = 0),  # b, 1, 28, 28\n",
        "            #nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIeZ7JTCgLz5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c175444-5c9a-43c8-9e34-a5a544c65006"
      },
      "source": [
        "### Train an autoencoder\n",
        "\n",
        "num_epochs = 100\n",
        "batch_size = 1\n",
        "learning_rate = 1e-3\n",
        "\n",
        "model = autoencoder()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "total_loss = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for data in dataloader:\n",
        "        img = Variable(data)\n",
        "        # ===================forward=====================\n",
        "        output = model(img)\n",
        "        loss = criterion(output, img)\n",
        "        # ===================backward====================\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # ===================log========================\n",
        "    total_loss += loss.data\n",
        "    print('epoch [{}/{}]'\n",
        "          .format(epoch+1, num_epochs))\n",
        "    if epoch % 10 == 0:\n",
        "        pic = output.cpu()#.detach()#.numpy()\n",
        "        save_image(pic, 'image_{}.png'.format(epoch))\n",
        "\n",
        "torch.save(model.state_dict(), './conv_autoencoder.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:126: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [1/50], loss:0.0145\n",
            "epoch [2/50], loss:0.0248\n",
            "epoch [3/50], loss:0.0335\n",
            "epoch [4/50], loss:0.0402\n",
            "epoch [5/50], loss:0.0466\n",
            "epoch [6/50], loss:0.0526\n",
            "epoch [7/50], loss:0.0583\n",
            "epoch [8/50], loss:0.0637\n",
            "epoch [9/50], loss:0.0689\n",
            "epoch [10/50], loss:0.0737\n",
            "epoch [11/50], loss:0.0784\n",
            "epoch [12/50], loss:0.0830\n",
            "epoch [13/50], loss:0.0875\n",
            "epoch [14/50], loss:0.0919\n",
            "epoch [15/50], loss:0.0962\n",
            "epoch [16/50], loss:0.1004\n",
            "epoch [17/50], loss:0.1047\n",
            "epoch [18/50], loss:0.1089\n",
            "epoch [19/50], loss:0.1131\n",
            "epoch [20/50], loss:0.1172\n",
            "epoch [21/50], loss:0.1213\n",
            "epoch [22/50], loss:0.1254\n",
            "epoch [23/50], loss:0.1295\n",
            "epoch [24/50], loss:0.1336\n",
            "epoch [25/50], loss:0.1377\n",
            "epoch [26/50], loss:0.1417\n",
            "epoch [27/50], loss:0.1457\n",
            "epoch [28/50], loss:0.1498\n",
            "epoch [29/50], loss:0.1538\n",
            "epoch [30/50], loss:0.1578\n",
            "epoch [31/50], loss:0.1618\n",
            "epoch [32/50], loss:0.1658\n",
            "epoch [33/50], loss:0.1698\n",
            "epoch [34/50], loss:0.1737\n",
            "epoch [35/50], loss:0.1777\n",
            "epoch [36/50], loss:0.1817\n",
            "epoch [37/50], loss:0.1856\n",
            "epoch [38/50], loss:0.1896\n",
            "epoch [39/50], loss:0.1935\n",
            "epoch [40/50], loss:0.1975\n",
            "epoch [41/50], loss:0.2014\n",
            "epoch [42/50], loss:0.2053\n",
            "epoch [43/50], loss:0.2093\n",
            "epoch [44/50], loss:0.2132\n",
            "epoch [45/50], loss:0.2171\n",
            "epoch [46/50], loss:0.2210\n",
            "epoch [47/50], loss:0.2250\n",
            "epoch [48/50], loss:0.2289\n",
            "epoch [49/50], loss:0.2328\n",
            "epoch [50/50], loss:0.2367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpuX4uRbgXWF"
      },
      "source": [
        "### Check the reconstructed output by feeding the same input\n",
        "\n",
        "image = plt.imread(IMG_URL)\n",
        "image = TF.to_tensor(image)\n",
        "image = image.unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_35hE-MAgc3y"
      },
      "source": [
        "predicted = model(image)\n",
        "save_image(predicted, 'predicted.png')\n",
        "\n",
        "# show image the predicted image\n",
        "predicted = predicted.detach().squeeze().permute(1,2,0)\n",
        "plt.axis('off')\n",
        "plt.imshow(predicted)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kq7Z94nhF49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "122bdf12-8443-43fc-958a-caf57a32a99a"
      },
      "source": [
        "### Start pruning\n",
        "\n",
        "print(model.state_dict().keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['encoder.0.weight', 'encoder.0.bias', 'encoder.2.weight', 'encoder.2.bias', 'decoder.0.weight', 'decoder.0.bias', 'decoder.2.weight', 'decoder.2.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8E-mtWihJbd"
      },
      "source": [
        "new_model = model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-vj1je-hLR7"
      },
      "source": [
        "\n",
        "PRUNING_AMOUNT = 0.2\n",
        "\n",
        "for name, module in new_model.named_modules():\n",
        "    # prune 10% of connections in all 2D-conv layers \n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "       prune.l1_unstructured(module, name='weight', amount=PRUNING_AMOUNT)\n",
        "    # prune 10% of connections in all ConvTranspose2d layers \n",
        "    if isinstance(module, torch.nn.ConvTranspose2d):\n",
        "       prune.l1_unstructured(module, name='weight', amount=PRUNING_AMOUNT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo848lvXhT_u"
      },
      "source": [
        "pruned20 = new_model(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scm1yvUshaAB"
      },
      "source": [
        "save_image(pruned20, 'pruned20.png')\n",
        "pruned2 = pruned2.detach().squeeze().permute(1,2,0)\n",
        "plt.axis('off')\n",
        "plt.imshow(pruned2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}